{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Module 3 - TensorFlow Mechanics 101\n",
    "\n",
    "\n",
    "This is step-by-step follow along based on Google's Tensorflow Tutorial.\n",
    "\n",
    "[TensorFlow Mechanics 101](https://www.tensorflow.org/get_started/mnist/mechanics)\n",
    "\n",
    "The tutorial is structured to go over python codes which was supposed to be run from the command line:\n",
    "\n",
    "python fully_connected_feed.py\n",
    "\n",
    "But there are many Python and TensorFlow programming concepts which I want to explore in greater detail. So I am re-doing this section in Jupyter Notebook.\n",
    "\n",
    "Section 1 deals primarily with Python's argparse module, which is important for writing user-friendly Python scripts run from the command line.\n",
    "\n",
    "This section essentially takes apart the command line codes provided by Google and rebuild it from the ground up. A lot of attention is paid to getting TensorBoard to work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "\n",
    "# These 3 lines provides backward compatibility with older Python versions from Python 3 code\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# six is a package that helps in writing code that is compatible with both Python 2 and Python 3.\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tempfile\n",
    "import argparse\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# The mnist read_data_sets() function will be used in full_connected_feed.py to download mnist dataset\n",
    "# to your local training folder and to then unpack that data to return a dictionary of DataSet instances.\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SECTION 2 - Code Organization\n",
    "\n",
    "The command line codes provided by Google is structured as followed:\n",
    "\n",
    "\n",
    "|code| what it does |\n",
    "|----|:----:|\n",
    "|input_data.py  | import statements for downloading and reading MNIST data |\n",
    "|mnist.py | The code that builds the computation graph for a fully-connected MNIST model |\n",
    "|fully_connected_feed.py |The main code to train the built MNIST model against the downloaded dataset using a feed dictionary |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## input_data.py\n",
    "\n",
    "The code is a bunch of import statements. The first half deals with Python version compatibility. The second half imports libraries such as numpy, tensorflow, read_data_set, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "\n",
    "# These 3 lines provides backward compatibility with older Python versions from Python 3 code\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# six is a package that helps in writing code that is compatible with both Python 2 and Python 3.\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# The mnist read_data_sets() function will be used in full_connected_feed.py to download mnist dataset\n",
    "# to your local training folder and to then unpack that data to return a dictionary of DataSet instances.\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## mnist.py and fully_connected_feed.py\n",
    "\n",
    "The computation graph is built from the **mnist.py** file according to a 3-stage pattern: inference(), loss(), and training().\n",
    "\n",
    "    inference() - Builds the graph as far as is required for running the network forward to make predictions.\n",
    "    loss() - Adds to the inference graph the ops required to generate loss.\n",
    "    training() - Adds to the loss graph the ops required to compute and apply gradients.\n",
    "\n",
    "**fully_connected_feed.py** trains the built MNIST model against the downloaded dataset using a feed dictionary. It is written to be run from the command line \n",
    "\n",
    "```sh\n",
    "$ python fully_connected_feed.py\n",
    "\n",
    "$ python3 fully_connected_feed.py --help\n",
    "usage: fully_connected_feed.py [-h] [--learning_rate LEARNING_RATE]\n",
    "                               [--max_steps MAX_STEPS] [--hidden1 HIDDEN1]\n",
    "                               [--hidden2 HIDDEN2] [--batch_size BATCH_SIZE]\n",
    "                               [--input_data_dir INPUT_DATA_DIR]\n",
    "                               [--log_dir LOG_DIR] [--fake_data]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --learning_rate LEARNING_RATE\n",
    "                        Initial learning rate.\n",
    "  --max_steps MAX_STEPS\n",
    "                        Number of steps to run trainer.\n",
    "  --hidden1 HIDDEN1     Number of units in hidden layer 1.\n",
    "  --hidden2 HIDDEN2     Number of units in hidden layer 2.\n",
    "  --batch_size BATCH_SIZE\n",
    "                        Batch size. Must divide evenly into the dataset sizes.\n",
    "  --input_data_dir INPUT_DATA_DIR\n",
    "                        Directory to put the input data.\n",
    "  --log_dir LOG_DIR     Directory to put the log data.\n",
    "  --fake_data           If true, uses fake data for unit testing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## fully_connected_feed.py\n",
    "\n",
    "We first work with fully_connected_feed.py.\n",
    "\n",
    "First the import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (1) Populate Model Parameters\n",
    "\n",
    "Here we use argparse to populate model parameters from command-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Basic model parameters as external flags.\n",
    "FLAGS = None\n",
    "\n",
    "# Without a program name, ArgumentParser determine the command-line arguments from sys.argv\n",
    "parser = argparse.ArgumentParser()\n",
    "    \n",
    "parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.01,\n",
    "      help='Initial learning rate.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "      '--max_steps',\n",
    "      type=int,\n",
    "      default=10000,\n",
    "      help='Number of steps to run trainer.'\n",
    ")\n",
    "    \n",
    "parser.add_argument(\n",
    "      '--hidden1',\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help='Number of units in hidden layer 1.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "      '--hidden2',\n",
    "      type=int,\n",
    "      default=32,\n",
    "      help='Number of units in hidden layer 2.'\n",
    ")\n",
    "    \n",
    "parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Batch size.  Must divide evenly into the dataset sizes.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "      '--input_data_dir',\n",
    "      type=str,\n",
    "      default='/tmp/tensorflow/mnist/input_data',\n",
    "      help='Directory to put the input data.'\n",
    ")\n",
    "    \n",
    "parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default='/home/lukeliem/TensorFlow/logs/fully_connected_feed',\n",
    "      help='Directory to put the log data.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "      '--fake_data',\n",
    "      default=False,\n",
    "      help='If true, uses fake data for unit testing.',\n",
    "      action='store_true'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sometimes a script may only parse a few of the command-line arguments, passing the remaining arguments on to another \n",
    "# script or program. parse_known_args() returns a two item tuple containing the populated namespace (into FLAG) and the\n",
    "# list of remaining argument strings.\n",
    "FLAGS, unparsed = parser.parse_known_args(['--max_steps','10000', '--learning_rate','0.001'])\n",
    "\n",
    "# FLAGS is the Namespace which stores all the parameters\n",
    "print (FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2) Output Graph --> TensorBoard\n",
    "\n",
    "The commands below output the computation graph to TensorBoard.\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir='./logs/fully_connected_feed'\n",
    "```\n",
    "\n",
    "Go to URL \"http://localhost:6006/\"\n",
    "\n",
    "![2 placeholders](images/Screenshot from 2017-06-15 15-27-06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.close()  # Always remember to close the summary writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (3) Build the Inference Engine\n",
    "\n",
    "The inference engine performs inferences (predictions). It takes the images placeholder as input and builds on top of it a pair of fully connected layers (hidden1 and hidden2) with ReLU activation followed by a ten node linear layer (softmax_linear) specifying the output logits.\n",
    "\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "TensorBoard graph after adding the first hidden layer:\n",
    "\n",
    "![2 placeholders + 1 hidden layer ](images/Screenshot from 2017-06-15 17-23-58.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "    \n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.close()  # Always remember to close the summary writer    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TensorBoard graph after adding the inference engine:\n",
    "\n",
    "![Inference Engine](images/Screenshot from 2017-06-15 17-44-00.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden2'):\n",
    "    # \"hidden2/weights\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "        name='weights')\n",
    "    # \"hidden2/biases\"\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "# Linear\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    # \"softmax_linear/weights\"    \n",
    "    weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "    # \"softmax_linear/biases\" \n",
    "    biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "   \n",
    "\n",
    "# Add the variable initializer Op.\n",
    "tf.global_variables_initializer()  \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.close()  # Always remember to close the summary writer  \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (4) Build the Loss Function\n",
    "\n",
    "Here we add the loss function to the inference engine.\n",
    "\n",
    "![Inference + Loss](images/Screenshot from 2017-06-19 14-20-42.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "'''\n",
    "The Inference Engine\n",
    "'''\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden2'):\n",
    "    # \"hidden2/weights\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "        name='weights')\n",
    "    # \"hidden2/biases\"\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "# Linear\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    # \"softmax_linear/weights\"    \n",
    "    weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "    # \"softmax_linear/biases\" \n",
    "    biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "'''\n",
    "The Loss Function\n",
    "'''\n",
    "\n",
    "labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "# This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "# inference engine\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "# This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "\n",
    "\n",
    "# Add the variable initializer Op.\n",
    "tf.global_variables_initializer()  \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "summary_writer.flush()\n",
    "summary_writer.close()  # Always remember to close the summary writer \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (5) Build the Training Op\n",
    "\n",
    "Here we add the Training Op using the ADAM Optimizer.\n",
    "\n",
    "![Inference + Loss + Train](images/Screenshot from 2017-06-19 14-52-37.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "'''\n",
    "The Inference Engine\n",
    "'''\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden2'):\n",
    "    # \"hidden2/weights\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "        name='weights')\n",
    "    # \"hidden2/biases\"\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "# Linear\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    # \"softmax_linear/weights\"    \n",
    "    weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "    # \"softmax_linear/biases\" \n",
    "    biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "'''\n",
    "The Loss Function\n",
    "'''\n",
    "\n",
    "labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "# This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "# inference engine\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "# This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "'''\n",
    "The Training Op\n",
    "'''\n",
    "# tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "# tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "# summaries are written out.\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Create the gradient descent optimizer with the given learning rate.\n",
    "optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "# Create a variable to track the global step.\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# Use the optimizer to apply the gradients that minimize the loss\n",
    "# (and also increment the global step counter) as a single training step.\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "# Add the variable initializer Op.\n",
    "tf.global_variables_initializer()  \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.close()  # Always remember to close the summary writer \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (6) Evaluation\n",
    "\n",
    "Add the graph for the Evaluation Op which valuates the quality of the logits at predicting the label. Also, define name_scope for various ops and layers, so that the graph is more readable.\n",
    "\n",
    "![Inference + Loss + Train + Eval](images/Screenshot from 2017-06-19 16-46-45.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# InteractiveSession class allows you to interleave operations which build a computation graph with ones that\n",
    "# run the graph. This is particularly convenient when working in interactive contexts like IPython.\n",
    "# Otherwise, we should build the entire computation graph before starting a session and launching the graph.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "'''\n",
    "The Inference Engine\n",
    "'''\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden2'):\n",
    "    # \"hidden2/weights\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "        name='weights')\n",
    "    # \"hidden2/biases\"\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "# Linear\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    # \"softmax_linear/weights\"    \n",
    "    weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "    # \"softmax_linear/biases\" \n",
    "    biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "'''\n",
    "The Loss Function\n",
    "'''\n",
    "\n",
    "with tf.name_scope('softmax'):\n",
    "    labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "    # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "    # inference engine\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "    # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "'''\n",
    "The Training Op\n",
    "'''\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "\n",
    "    # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "    # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "    # summaries are written out.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "'''\n",
    "The Evaluation Op\n",
    "'''\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "    # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "    # Return the number of true entries.\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "# Add the variable initializer Op.\n",
    "tf.global_variables_initializer()  \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()  # Always remember to close the summary writer \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (7) The Session\n",
    "\n",
    "Once the graph has been completed and all of the necessary ops generated, a tf.Session is created for running the graph.\n",
    "\n",
    "Note that we stop using tf.Interactive_Session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # Delete everything in the log directory if it already exists\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "# verify the dimension of the placeholders\n",
    "print (images_placeholder.get_shape())\n",
    "print (labels_placeholder.get_shape())\n",
    "\n",
    "'''\n",
    "The Inference Engine\n",
    "'''\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden1'):\n",
    "    # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "    weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "        stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "    # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden2'):\n",
    "    # \"hidden2/weights\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "        name='weights')\n",
    "    # \"hidden2/biases\"\n",
    "    biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "# Linear\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    # \"softmax_linear/weights\"    \n",
    "    weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "    # \"softmax_linear/biases\" \n",
    "    biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "'''\n",
    "The Loss Function\n",
    "'''\n",
    "\n",
    "with tf.name_scope('softmax'):\n",
    "    labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "    # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "    # inference engine\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "    # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "'''\n",
    "The Training Op\n",
    "'''\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "\n",
    "    # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "    # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "    # summaries are written out.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "'''\n",
    "The Evaluation Op\n",
    "'''\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "    # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "    # Return the number of true entries.\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "\n",
    "# Add the variable initializer Op.\n",
    "init = tf.global_variables_initializer()  \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all() \n",
    "\n",
    "'''\n",
    "The Session\n",
    "'''\n",
    "# Create a session for running Ops on the Graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the Op to initialize the variables.\n",
    "sess.run(init)\n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "\n",
    "summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (8) The Train Loop\n",
    "\n",
    "We get out of using Interactive_Session and use the regular tf_Session() instead.\n",
    "\n",
    "There appears to be a bug in TensorFlow which makes the output of summary data and graphs to TensorBoard very unreliable. We have to work around as followed:\n",
    "\n",
    "* To get a correct graph representation, stop tensorboard and jupyter, delete the tensorflow logdir, restart jupyter, run the script, and then restart tensorboard.  \n",
    "\n",
    "[Imanol Schlag's Blog on Using TensorBoard](http://ischlag.github.io/2016/06/04/how-to-use-tensorboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # If log directory exists, delete everything in it\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "    # verify the dimension of the placeholders\n",
    "    print (images_placeholder.get_shape())\n",
    "    print (labels_placeholder.get_shape())\n",
    "\n",
    "    '''\n",
    "    The Inference Engine\n",
    "    '''\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "        weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "            stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "        # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        # \"hidden2/weights\"\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "            name='weights')\n",
    "        # \"hidden2/biases\"\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        # \"softmax_linear/weights\"    \n",
    "        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        # \"softmax_linear/biases\" \n",
    "        biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    '''\n",
    "    The Loss Function\n",
    "    '''\n",
    "    with tf.name_scope('softmax'):\n",
    "        labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "        # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "        # inference engine\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "        # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "    '''\n",
    "    The Training Op\n",
    "    '''\n",
    "    with tf.name_scope('ADAM'):\n",
    "\n",
    "        # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "        # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "        # summaries are written out.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Create the gradient descent optimizer with the given learning rate.\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        # Create a variable to track the global step.\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Use the optimizer to apply the gradients that minimize the loss\n",
    "        # (and also increment the global step counter) as a single training step.\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('eval'):\n",
    "        # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "        # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "        # Return the number of true entries.\n",
    "        eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all() \n",
    "\n",
    "    '''\n",
    "    The Session\n",
    "    '''\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    '''\n",
    "    The Train Loop\n",
    "    '''\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "        # Create the feed_dict for the placeholders filled with the next\n",
    "        # `batch size` examples.\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  \n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        # Print an overview every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f' % (step, loss_value))\n",
    "\n",
    "    summary_writer.flush()  # Always remember to close the summary writer\n",
    "    summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (9) Using tf.Summary\n",
    "\n",
    "These tf.summary commands are used to output summary and graph to TensorBoard:\n",
    "\n",
    "```python\n",
    "        # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "        # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "        # summaries are written out.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    # All the summaries (in this case, only tf.summary.scalar('loss', loss)) are collected into a single Tensor \n",
    "    # during the graph building phase.\n",
    "    summary = tf.summary.merge_all() \n",
    "    \n",
    "    # After the session is created, a tf.summary.FileWriter is instantiated to write the events files, which \n",
    "    # contain both the graph and the values of the summaries. \n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "        \n",
    "            # The events file is updated with new summary values every time the summary is evaluated \n",
    "            # and the output passed to the writer's add_summary() function.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()    \n",
    "            \n",
    "    summary_writer.close()  # Always remember to close the summary writer \n",
    "    \n",
    "```       \n",
    "\n",
    "![TensorBoard Scalar](images/Screenshot from 2017-06-20 16-17-33.png)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # If log directory exists, delete everything in it\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "    # verify the dimension of the placeholders\n",
    "    print (images_placeholder.get_shape())\n",
    "    print (labels_placeholder.get_shape())\n",
    "\n",
    "    '''\n",
    "    The Inference Engine\n",
    "    '''\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "        weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "            stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "        # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        # \"hidden2/weights\"\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "            name='weights')\n",
    "        # \"hidden2/biases\"\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        # \"softmax_linear/weights\"    \n",
    "        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        # \"softmax_linear/biases\" \n",
    "        biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    '''\n",
    "    The Loss Function\n",
    "    '''\n",
    "    with tf.name_scope('softmax'):\n",
    "        labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "        # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "        # inference engine\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "        # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "    '''\n",
    "    The Training Op\n",
    "    '''\n",
    "    with tf.name_scope('ADAM'):\n",
    "\n",
    "        # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "        # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "        # summaries are written out.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Create the gradient descent optimizer with the given learning rate.\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        # Create a variable to track the global step.\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Use the optimizer to apply the gradients that minimize the loss\n",
    "        # (and also increment the global step counter) as a single training step.\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('eval'):\n",
    "        # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "        # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "        # Return the number of true entries.\n",
    "        eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    # All the summaries (in this case, only tf.summary.scalar('loss', loss)) are collected into a single Tensor \n",
    "    # during the graph building phase.\n",
    "    summary = tf.summary.merge_all() \n",
    "\n",
    "    '''\n",
    "    The Session\n",
    "    '''\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # After the session is created, a tf.summary.FileWriter is instantiated to write the events files, which \n",
    "    # contain both the graph and the values of the summaries. \n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    '''\n",
    "    The Train Loop\n",
    "    '''\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "        # Create the feed_dict for the placeholders filled with the next\n",
    "        # `batch size` examples.\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  \n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        # Print an overview every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f' % (step, loss_value))\n",
    "            # The events file is updated with new summary values every time the summary is evaluated \n",
    "            # and the output passed to the writer's add_summary() function.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "    summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (10) Evaluation\n",
    "\n",
    "Every thousand steps, the code will attempt to evaluate the model against both the training, validation and test datasets. This is performed in the do_eval() function.\n",
    "\n",
    "Inside the function, there is a loop for filling a feed_dict and calling sess.run() against the eval_correct op to evaluate the model on the given dataset(training, validation or test)\n",
    "\n",
    "The eval_correct op simply generates a tf.nn.in_top_k op that automatically scores each model output as correct if the true label can be found in the K most-likely predictions (where k=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "    feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "    }    \n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "    \n",
    "\n",
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # If log directory exists, delete everything in it\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "    # verify the dimension of the placeholders\n",
    "    print (images_placeholder.get_shape())\n",
    "    print (labels_placeholder.get_shape())\n",
    "\n",
    "    '''\n",
    "    The Inference Engine\n",
    "    '''\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "        weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "            stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "        # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        # \"hidden2/weights\"\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "            name='weights')\n",
    "        # \"hidden2/biases\"\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        # \"softmax_linear/weights\"    \n",
    "        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        # \"softmax_linear/biases\" \n",
    "        biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    '''\n",
    "    The Loss Function\n",
    "    '''\n",
    "    with tf.name_scope('softmax'):\n",
    "        labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "        # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "        # inference engine\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "        # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "    '''\n",
    "    The Training Op\n",
    "    '''\n",
    "    with tf.name_scope('ADAM'):\n",
    "\n",
    "        # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "        # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "        # summaries are written out.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Create the gradient descent optimizer with the given learning rate.\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        # Create a variable to track the global step.\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Use the optimizer to apply the gradients that minimize the loss\n",
    "        # (and also increment the global step counter) as a single training step.\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('eval'):\n",
    "        # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "        # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "        # Return the number of true entries.\n",
    "        eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    # All the summaries (in this case, only tf.summary.scalar('loss', loss)) are collected into a single Tensor \n",
    "    # during the graph building phase.\n",
    "    summary = tf.summary.merge_all() \n",
    "\n",
    "    '''\n",
    "    The Session\n",
    "    '''\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # After the session is created, a tf.summary.FileWriter is instantiated to write the events files, which \n",
    "    # contain both the graph and the values of the summaries. \n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    '''\n",
    "    The Train Loop\n",
    "    '''\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "        # Create the feed_dict for the placeholders filled with the next\n",
    "        # `batch size` examples.\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  \n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        # Print an overview every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f' % (step, loss_value))\n",
    "            # The events file is updated with new summary values every time the summary is evaluated \n",
    "            # and the output passed to the writer's add_summary() function.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "        # Evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.train)\n",
    "            # Evaluate against the validation set.\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.validation)\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.test)\n",
    "\n",
    "    summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (11) Add 2nd Scalar Summary \"Accuracy\"\n",
    "\n",
    "In addition to \"loss\", we now add a second scalar summary \"Accuracy\" to track the percent of correct predictions per batch every 100 steps.\n",
    "\n",
    "![Graph](images/Screenshot from 2017-06-21 14-04-22.png)\n",
    "\n",
    "![TensorBoard Scalar](images/Screenshot from 2017-06-20 17-49-49.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # If log directory exists, delete everything in it\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "    # verify the dimension of the placeholders\n",
    "    print (images_placeholder.get_shape())\n",
    "    print (labels_placeholder.get_shape())\n",
    "\n",
    "    '''\n",
    "    The Inference Engine\n",
    "    '''\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "        weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "            stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "        # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        # \"hidden2/weights\"\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "            name='weights')\n",
    "        # \"hidden2/biases\"\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        # \"softmax_linear/weights\"    \n",
    "        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        # \"softmax_linear/biases\" \n",
    "        biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    '''\n",
    "    The Loss Function\n",
    "    '''\n",
    "    with tf.name_scope('softmax'):\n",
    "        labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "        # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "        # inference engine\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "        # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "    '''\n",
    "    The Training Op\n",
    "    '''\n",
    "    with tf.name_scope('ADAM'):\n",
    "      \n",
    "\n",
    "        # Create the gradient descent optimizer with the given learning rate.\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        # Create a variable to track the global step.\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Use the optimizer to apply the gradients that minimize the loss\n",
    "        # (and also increment the global step counter) as a single training step.\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('eval'):\n",
    "        # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "        # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "        # Return the number of true entries.\n",
    "        eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(tf.one_hot(labels, depth=10),1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "\n",
    "    # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "    # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "    # summaries are written out.\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    # All the summaries (in this case, only tf.summary.scalar('loss', loss)) are collected into a single Tensor \n",
    "    # during the graph building phase.\n",
    "    summary = tf.summary.merge_all() \n",
    "\n",
    "    '''\n",
    "    The Session\n",
    "    '''\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # After the session is created, a tf.summary.FileWriter is instantiated to write the events files, which \n",
    "    # contain both the graph and the values of the summaries. \n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    '''\n",
    "    The Train Loop\n",
    "    '''\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "        # Create the feed_dict for the placeholders filled with the next\n",
    "        # `batch size` examples.\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  \n",
    "        _, loss_value, accuracy_value = sess.run([train_op, loss, accuracy], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        # Print an overview every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.3f accuracy = %.3f' % (step, loss_value, accuracy_value))\n",
    "            # The events file is updated with new summary values every time the summary is evaluated \n",
    "            # and the output passed to the writer's add_summary() function.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "\n",
    "    summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (12) Just for FUN!!!\n",
    "\n",
    "Accuracy and Evaluation are two Ops that do more or less the same things.\n",
    "\n",
    "Since the entire model is structured to process batches of data from the datasets, it makes sense to just add a extra operation within Evaluation Op to get the accuracy measurement:\n",
    "\n",
    "```python\n",
    "prediction = tf.div(tf.cast(eval_correct, tf.float32), FLAGS.batch_size)\n",
    "```\n",
    "\n",
    "That way, we can remove the Accuracy Op altogether!!!\n",
    "\n",
    "![TensorBoard Graph](images/Screenshot from 2017-06-21 15-27-43.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "    feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "    }    \n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "    \n",
    "\n",
    "# Deal with the Log file - important for TensorBoard\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)  # If log directory exists, delete everything in it\n",
    "tf.gfile.MakeDirs(FLAGS.log_dir)  # Create the directory if it does not exist already\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and test on MNIST.\n",
    "data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "# Remove all nodes from default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, mnist.IMAGE_PIXELS), name = 'images')\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size),name = 'truth_label')\n",
    "\n",
    "    # verify the dimension of the placeholders\n",
    "    print (images_placeholder.get_shape())\n",
    "    print (labels_placeholder.get_shape())\n",
    "\n",
    "    '''\n",
    "    The Inference Engine\n",
    "    '''\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        # Created under the hidden1 scope, the unique name given to the weights variable would be \"hidden1/weights\".\n",
    "        weights = tf.Variable(tf.truncated_normal([mnist.IMAGE_PIXELS, FLAGS.hidden1],\n",
    "            stddev=1.0 / math.sqrt(float(mnist.IMAGE_PIXELS))), name='weights')\n",
    "        # Likewise, the unique name given to the biases variable would be \"hidden1/biases\".\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        # \"hidden2/weights\"\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))),\n",
    "            name='weights')\n",
    "        # \"hidden2/biases\"\n",
    "        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        # \"softmax_linear/weights\"    \n",
    "        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, mnist.NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        # \"softmax_linear/biases\" \n",
    "        biases = tf.Variable(tf.zeros([mnist.NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    '''\n",
    "    The Loss Function\n",
    "    '''\n",
    "    with tf.name_scope('softmax'):\n",
    "        labels = tf.to_int64(labels_placeholder) #typecasting in int64\n",
    "    \n",
    "        # This op produces 1-hot labels from the labels_placeholder and compare them against logits from the\n",
    "        # inference engine\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "\n",
    "        # This op averages the cross entropy values across the batch dimension (the first dimension) as the total loss.\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "    '''\n",
    "    The Training Op\n",
    "    '''\n",
    "    with tf.name_scope('ADAM'):\n",
    "      \n",
    "\n",
    "        # Create the gradient descent optimizer with the given learning rate.\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        # Create a variable to track the global step.\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Use the optimizer to apply the gradients that minimize the loss\n",
    "        # (and also increment the global step counter) as a single training step.\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, name='minimize')\n",
    "\n",
    "    '''\n",
    "    The Evaluation Op\n",
    "    '''\n",
    "    with tf.name_scope('eval'):\n",
    "        # For a classifier model, we can use the in_top_k Op. It returns a bool tensor with shape [batch_size] \n",
    "        # that is true for the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1, name = 'top_k')\n",
    "        # Return the number of true entries.\n",
    "        eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32), name = 'reduce_sum')\n",
    "        prediction = tf.div(tf.cast(eval_correct, tf.float32), FLAGS.batch_size)\n",
    "\n",
    "    # tf.summary.scalar is an op for generating summary values into the events file when used with a \n",
    "    # tf.summary.FileWriter. In this case, it will emit the snapshot value of the loss every time the\n",
    "    # summaries are written out.\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('prediction', prediction)\n",
    "    \n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    # All the summaries (in this case, only tf.summary.scalar('loss', loss)) are collected into a single Tensor \n",
    "    # during the graph building phase.\n",
    "    summary = tf.summary.merge_all() \n",
    "\n",
    "    '''\n",
    "    The Session\n",
    "    '''\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # After the session is created, a tf.summary.FileWriter is instantiated to write the events files, which \n",
    "    # contain both the graph and the values of the summaries. \n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    \n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    '''\n",
    "    The Train Loop\n",
    "    '''\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "\n",
    "        # Create the feed_dict for the placeholders filled with the next\n",
    "        # `batch size` examples.\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  \n",
    "        _, loss_value, prediction_value = sess.run([train_op, loss, prediction], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        # Print an overview every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.3f accuracy = %.3f' % (step, loss_value, prediction_value))\n",
    "            # The events file is updated with new summary values every time the summary is evaluated \n",
    "            # and the output passed to the writer's add_summary() function.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "        # Evaluate the model every 1000 steps\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.train)\n",
    "            # Evaluate against the validation set.\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.validation)\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.test)\n",
    "\n",
    "    summary_writer.close()  # Always remember to close the summary writer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (13) Parameter Optimization - Learning Rate\n",
    "\n",
    "Learning rate for the ADAM Optimizer is a key parameter that we can optimize:\n",
    "\n",
    "lr = 0.05 Test Data Precision: 0.94  \n",
    "lr = 0.01 Test Data Precision: 0.974  \n",
    "lr = 0.005 Test Data Precision: 0.976  \n",
    "lr = 0.002 Test Data Precision: 0.978   \n",
    "lr = 0.001 Test Data Precision: 0.977   \n",
    "lr = 0.0005 Test Data Precision: 0.977   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sometimes a script may only parse a few of the command-line arguments, passing the remaining arguments on to another \n",
    "# script or program. parse_known_args() returns a two item tuple containing the populated namespace (into FLAG) and the\n",
    "# list of remaining argument strings.\n",
    "FLAGS, unparsed = parser.parse_known_args(['--max_steps','10000', '--learning_rate','0.05'])\n",
    "\n",
    "# FLAGS is the Namespace which stores all the parameters\n",
    "print (FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
