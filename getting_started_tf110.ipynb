{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1 - Getting Started\n",
    "\n",
    "This is step-by-step follow along based on Google's Tensorflow Tutorial.\n",
    "\n",
    "[Getting Started With TensorFlow](https://www.tensorflow.org/get_started/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.1.0\n",
      "Pyhton version:  3.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Pyhton version: \", platform.python_version())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Separation - Graph vs Execution\n",
    "Notice that printing the nodes does not output the values 3.0 and 4. Instead, they are nodes that, when evaluated, would produce 3.0 and 4.0, respectively. To actually evaluate the nodes, we must run the computational graph within a session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_12:0\", shape=(), dtype=float32) Tensor(\"Const_13:0\", shape=(), dtype=float32)\n",
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "\n",
    "print(node1, node2)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder and Feeder\n",
    "A graph can be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to provide a value later.\n",
    "\n",
    "We can evaluate this graph with multiple inputs by using the feed_dict parameter to specify Tensors that provide concrete values to these placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n",
      "[[  3.   7.]\n",
      " [ 13.  12.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "# Placeholders allow multiple inputs to be feed into the same graph\n",
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))\n",
    "print(sess.run(adder_node, {a: [[1,3],[5,6]], b: [[2, 4],[8,6]]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n",
      "[  9.  21.]\n",
      "[[  9.  21.]\n",
      " [ 39.  36.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "add_and_triple = adder_node * 3.\n",
    "\n",
    "print(sess.run(add_and_triple, {a: 3, b:4.5}))\n",
    "print(sess.run(add_and_triple, {a: [1,3], b: [2, 4]}))\n",
    "print(sess.run(add_and_triple, {a: [[1,3],[5,6]], b: [[2, 4],[8,6]]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "In machine learning we will typically want a model that can take arbitrary inputs, such as x. And to make the model trainable, we need to be able to modify the graph to get new outputs with the same input. Variables allow us to add trainable parameters (W and b) to a graph and they are constructed with a type and initial value.\n",
    "\n",
    "Constants are initialized when you call tf.constant, and their value can never change. Variables are not initialized when you call tf.Variable. To initialize all the variables in a TensorFlow program, you must explicitly call the operation  tf.global_variables_initializer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "To evaluate the model on training data, we need a y placeholder to provide the truth labels, and we need to write a loss function.\n",
    "\n",
    "A loss function measures how far apart the current model is from the truth labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y = tf.placeholder(tf.float32)  # Placeholder for truth labels\n",
    "\n",
    "squared_deltas = tf.square(linear_model - y)   # loss function\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "The whole point of machine learning is to find the correct model parameters automatically. \n",
    "\n",
    "TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function. The simplest optimizer is gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)  # The good ol' SGD\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess.run(init) # reset values to incorrect defaults.\n",
    "\n",
    "# Run SGD for 1000 steps\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "\n",
    "print(sess.run([W, b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x:x_train, y:y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.contrib.learn\n",
    "\n",
    "tf.contrib.learn is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "    running training loops\n",
    "    running evaluation loops\n",
    "    managing data sets\n",
    "    managing feeding\n",
    "\n",
    "tf.contrib.learn defines many common models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5343056e-08, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# This gets rid of a lot of WARNINGs which are rather trivial\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use `numpy_input_fn`. We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4, num_epochs=1000)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the `fit` method and passing the\n",
    "# training data set.\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did. In a real example, we would want\n",
    "# to use a separate validation and testing data set to avoid overfitting.\n",
    "print(estimator.evaluate(input_fn=input_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "\n",
    "Suppose we wanted to create a custom model that is not built into TensorFlow. We can still retain the high level abstraction of data set, feeding, training, etc. of tf.contrib.learn. \n",
    "\n",
    "To define a custom model that works with tf.contrib.learn, we need to use tf.contrib.learn.Estimator. tf.contrib.learn.LinearRegressor is actually a sub-class of tf.contrib.learn.Estimator. Instead of sub-classing Estimator, we simply provide Estimator a function model_fn that tells tf.contrib.learn how it can evaluate predictions, training steps, and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model(features, labels, mode):\n",
    "  \n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "\n",
    "  # ModelFnOps connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.contrib.learn.ModelFnOps(\n",
    "      mode=mode, predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "\n",
    "# define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# evaluate our model\n",
    "print(estimator.evaluate(input_fn=input_fn, steps=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
